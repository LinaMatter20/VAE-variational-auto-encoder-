# -*- coding: utf-8 -*-
"""VAE_Tast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VskSE326AXHtWj2-MrPQQ3MQ6tVjlXXr

On MNIST hand-written digits dataset

    Binarize the pics

    Apply VAE
"""

#imports
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms

# Settings

LATENT_SPACE_SIZE = 2
BATCH_SIZE = 128
NUMBER_OF_EPOCHS = 10
LEARNING_RATE = 0.001

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("We are using device:", DEVICE)

# Load MNIST and prepare data
transform = transforms.ToTensor()

train_dataset = torchvision.datasets.MNIST(
    root="mnist_data",
    train=True,
    download=True,
    transform=transform
)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True)

"""# logic of training steps:


*   Feed binary image to encoder → get μ, log_var.
*   Sample z
*   Decode z → recon image
*   Compute losses
*   Backprop and update weights


"""

# Define the VAE model

class SimpleConvVAE(nn.Module):

    def __init__(self, latent_dim=2):
        super().__init__()

        #   ENCODER part
        # We use convolutional layers because images have spatial structure
        self.encoder_convs = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
        )
        # After convolutions we have 64 feature maps of size 7×7
        self.flatten = nn.Flatten()  # turns it into vector of size 64*7*7 = 3136

        self.fc_before_mu_logvar = nn.Linear(64 * 7 * 7, 256)
        self.relu_middle = nn.ReLU()

        self.layer_for_mean     = nn.Linear(256, latent_dim)
        self.layer_for_log_var  = nn.Linear(256, latent_dim)

        #   DECODER part
        self.decoder_start = nn.Linear(latent_dim, 64 * 7 * 7)
        self.relu_decoder = nn.ReLU()

        self.decoder_convs = nn.Sequential(
            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()   #-> output between (0,1)
        )

    def encoder_forward(self, x):
        # x shape: [batch, 1, 28, 28]
        features = self.encoder_convs(x)
        flat = self.flatten(features)
        hidden = self.fc_before_mu_logvar(flat)
        hidden = self.relu_middle(hidden)

        mean_vector     = self.layer_for_mean(hidden)
        log_variance    = self.layer_for_log_var(hidden)

        return mean_vector, log_variance

    def reparameterization_trick(self, mean, log_var):
        # This is the magic sampling part
        std_dev = torch.exp(0.5 * log_var)
        random_noise = torch.randn_like(std_dev)   # same shape as std_dev
        z_sample = mean + random_noise * std_dev
        return z_sample

    def decoder_forward(self, z):
        # z shape: [batch, latent_dim]
        expanded = self.decoder_start(z)
        expanded = self.relu_decoder(expanded)

        # Make it image shape again: [batch, 64, 7, 7]
        reshaped = expanded.view(-1, 64, 7, 7)

        reconstructed_image = self.decoder_convs(reshaped)
        return reconstructed_image

    def forward(self, input_images):
        mu, logvar = self.encoder_forward(input_images)
        z = self.reparameterization_trick(mu, logvar)
        reconstructed = self.decoder_forward(z)
        return reconstructed, mu, logvar


# Loss function

def compute_vae_loss(reconstructed, original, mu, logvar):

    # We use Binary Cross Entropy because images are 0/1
    recon_loss = nn.functional.binary_cross_entropy(
        reconstructed,
        original,
        reduction='sum'
    )

    # Average over batch
    recon_loss = recon_loss / original.shape[0]

    # KL divergence which makes distribution close to normal
    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
    kl_div = kl_div.mean()   # average over batch

    total_loss = recon_loss + kl_div

    return total_loss, recon_loss, kl_div

# Training loop

def train_model():

    model = SimpleConvVAE(latent_dim=LATENT_SPACE_SIZE)
    model = model.to(DEVICE)

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    print("Starting training...\n")

    for epoch in range(1, NUMBER_OF_EPOCHS + 1):

        model.train()
        total_epoch_loss = 0
        total_recon = 0
        total_kl = 0
        num_batches = 0

        for images, _ in train_loader:

            images = images.to(DEVICE)

            # Binarize [simple threshold]
            binary_images = (images > 0.5).float()

            optimizer.zero_grad()

            recon, mean, log_var = model(binary_images)

            loss, recon_part, kl_part = compute_vae_loss(
                recon, binary_images, mean, log_var
            )

            loss.backward()
            optimizer.step()

            total_epoch_loss += loss.item()
            total_recon += recon_part.item()
            total_kl += kl_part.item()
            num_batches += 1

        avg_loss = total_epoch_loss / num_batches
        avg_recon = total_recon / num_batches
        avg_kl = total_kl / num_batches

        print(f"Epoch {epoch:2d} | loss: {avg_loss:8.3f} | recon: {avg_recon:7.2f} | KL: {avg_kl:6.3f}")

    print("\nTraining finished!")
    return model

if __name__ == "__main__":
    trained_model = train_model()

"""# interpreatation for the result:

loss => total loss (the number the model is trying to make smaller)

recon => reconstruction loss (how badly the model is failing to rebuild the digits)

KL => KL divergence (how far the learned latent distributions are from a nice standard normal distribution)


Main observations:

Total loss is going down → the model is learning (good!)

Reconstruction loss is the biggest part and it keeps dropping → the model is getting better at drawing digits that look like the real ones

KL starts low, then slowly increases and stabilizes around ~6.6–6.7
"""

